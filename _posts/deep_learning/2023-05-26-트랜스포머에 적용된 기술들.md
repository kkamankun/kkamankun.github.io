---
title: "트랜스포머에 적용된 기술들"
categories:
    - Deep Learning
# layout: default
---
트랜스포머 블록은 멀티 헤드 어텐션, 피드 포워드 뉴럴 네트워크, 잔차 연결 및 레이어 정규화 등 세 가지 구성 요소를 기본으로 합니다.

FeedForward: 피드 포워드 뉴럴 네트워크

멀티 헤드 어텐션의 출력은 입력 단어들에 대응하는 벡터 시퀀스입니다. 이후 벡터 각각을 피드 포워드 뉴럴 네트워크에 입력합니다. 다시 말해 피드 포워드 뉴럴 네트워크의 입력은 현재 블록의 멀티 헤드 어텐션의 개별 출력 벡터가 됩니다.

피드 포워드 뉴럴 네트워크란 신경망의 한 종류로 입력(input layer, x), 은닉층(hidden layer, h), 출력층(output layer, y) 3개의 계층으로 구성돼 있습니다.

현재 트랜스포머에서는 은닉층의 뉴런 개수(즉, 은닉층의 차원수)를 입력층의 네 배로 설정하고 있습니다. 예를 들어, 피드 포워드 뉴럴 네트워크의 입력 벡터가 768차원일 경우 은닉층은 2048차원까지 늘렸다가 출력층에서 이를 다시 768차원으로 줄입니다.

Add: 잔차 연결

트랜스포머 블록의 Add는 잔차 연결(residual connection)을 가리킵니다. 잔차 연결이란 블록(block) 계산을 건너뛰는 경로를 하나 두는 것을 말합니다. 입력을 $x$, 이번 계산 대상 블록을  $F$라고 할 때 잔차 연결은 $F(x)+x$로 간단히 실현합니다. 모델이 다양한 관점에서 블록 계산을 수행하게 됩니다.

딥러닝 모델은 레이어가 많아지면 학습이 어려운 경향이 있습니다. 모델을 업데이트하기 위한 신호(그래디언트)가 전달되는 경로가 길어지기 때문입니다. 잔차 연결은 모델 중간에 블록을 건너뛰는 경로를 설정함으로써 학습을 용이하게 하는 효과까지 거둘 수 있습니다.

Norm: 레이어 정규화

레이어 정규화(layer normalization)란 미니 배치의 인스턴스(x)별로 평균을 빼주고 표준편차로 나눠 정규화(normalization)를 수행하는 기법입니다. 레이어 정규화를 수행하면 학습이 안정되고 그 속도가 빨라지는 등의 효과가 있다고 합니다.

딥러닝 모델은 그 표현력이 아주 좋아서 학습 데이터 그 자체를 외워버릴 염려가 있습니다. 이를 과적합(overfitting)이라고 합니다. 드롭아웃(dropout)은 이러한 과적합 현상을 방지하고자 뉴런의 일부를 확률적으로 0으로 대치하여 계산에서 제외하는 기법입니다. 드롭아웃은 학습 과정에만 적용하고, 학습이 끝나고 나서 인퍼런스 과정에서는 적용하지 않습니다.

트랜스포머 모델이 쓰는 최적화 도구는 아담 옵티마이저(Adam Optimizer)입니다. 아담 옵티마이저는 오차를 줄이는 성능이 좋아서 트랜스포머 말고도 널리 쓰이고 있습니다. 방향을 정할 때는 현재 위치에서 가장 경사가 급한 쪽으로 내려가되, 여태까지 내려오던 관성(방향)을 일부 유지하도록 합니다. 보폭의 경우 안 가본 곳은 성큼 빠르게 걸어 훑고 많이 가본 곳은 갈수록 보폭을 줄여 세밀하게 탐색하는 방식으로 정합니다.

---

참고문헌

[1] [ratsgo’s blog](https://ratsgo.github.io/nlpbook/docs/language_model/transformers/){: target="_blank"}