---
title: "취득한 RGB-NIR 데이터셋에 대한 검증"
categories:
    - Deep Learning
# layout: default
---
연구실 내에서 자체적으로 취득한 데이터셋을 검증했습니다. 취득한 데이터셋은 NIR-to-RGB와 RGB-to-NIR conversion 연구에 사용됩니다. Public 데이터셋을 사용하지 않고 자체적으로 데이터셋을 취득하는 이유는 다양합니다. 데이터셋을 직접 취득하면, task에 대한 이해도를 높일 수 있습니다. 딥러닝은 데이터 기반으로 학습하기 때문에 데이터를 직접 취득하면서 task에 대한 이해도를 높이는 것이 중요합니다. 또한 연구실에서 새롭게 시도하는 연구에 적합한 public 데이터셋이 없기 때문에 새로운 연구를 위해 데이터셋을 취득하는 과정이 필요합니다.

취득한 데이터셋의 검증 절차는 중요합니다. 데이터가 편향될 가능성이 높으며, 저품질 (예: saturation, blurring)의 영상 데이터가 섞여 있을 수 있습니다. 검증을 위해 한 가지 실험을 진행했습니다. 하나의 네트워크로 다양한 데이터셋을 학습시켜 결과를 비교했습니다.

네트워크는 U-Net으로 선택했습니다. 이 네트워크는 NIR-to-RGB conversion을 수행합니다. 실험에 사용된 데이터셋은 총 5 종류입니다. 자체 취득한 3 종류의 데이터셋과 2 종류의 public 데이터셋으로 구분됩니다.

자체 취득한 3 종류의 데이터셋은 indoor, outdoor, mix입니다. Indoor dataset은 연구실 내에서 카메라의 위치와 설정을 고정하여 물건만 바꿔서 scene을 다양하게 구성하여 취득했습니다. 총 236장입니다. 반면, outdoor dataset은 연구실 밖에서 취득했으며, 카메라의 위치와 설정이 상황에 따라 바뀌며 이에 따라 scene은 다양하게 바뀝니다. 총 269장입니다. 취득한 영상은 white balance가 조정되었고 register는 별도로 진행하지 않았습니다. Mix dataset은 indoor와 outdoor dataset의 혼합입니다.

2종류의 public 데이터셋은 VCIP, RGB-NIR scene입니다. VCIP dataset은 country, field, forest, mountain 총 4개의 클래스로 구성됩니다. 모든 영상의 해상도는 256x256이며, 총 400장입니다. RGB-NIR scene dataset은 VCIP dataset를 구성하는 클래스에 추가적으로 indoor, old building, street, urban, water로 총 9개의 클래스로 구성됩니다. 영상의 해상도는 영상마다 다르며, 총 477장입니다. 취득 후 영상은 white balance와 register되었습니다. 영상이 register가 되었음에도 불구하고, 완벽히 align이 맞지 않습니다.

![RGB-NIR scene dataset](https://github.com/kkamankun/kkamankun.github.io/assets/46318721/07801df1-8c12-41a9-97c3-2d6de61c1b3e){: width="80%"",height="80%""}

모델 학습을 할 때는 overfitting을 방지하기 위해 validation loss가 30 epoch 동안 감소하지 않으면 학습이 자동으로 종료됩니다(early stopping). 아래 그림은 training loss입니다. 모든 종류의 데이터셋에 대해 epoch가 증가함에 따라 loss가 감소합니다.

![training loss](https://github.com/kkamankun/kkamankun.github.io/assets/46318721/e07e6a38-0f81-40cc-9682-6b76915f5689)

아래 그림은 validation loss입니다. 연구실에서 자체적으로 취득한 데이터셋이 public 데이터셋보다 빠르게 overfitting이 발생합니다.

![validation loss](https://github.com/kkamankun/kkamankun.github.io/assets/46318721/134a33ad-9348-4674-a721-f8a003dfa544)

연구실에서 취득한 데이터셋에 대한 테스트 결과는 다음과 같습니다. 아래 그림은 indoor 데이터셋이며, epoch 51까지 학습한 모델로 예측한 영상입니다.

![Indoor test](https://github.com/kkamankun/kkamankun.github.io/assets/46318721/42a9637c-c3ff-4e57-83ec-d451977fee52){: width="50%"",height="50%""}


다음은 public 데이터셋에 대한 테스트 결과입니다. 아래 그림은 VCIP 데이터셋이며, epoch 111까지 학습한 모델로 예측한 영상입니다.

![VCIP test](https://github.com/kkamankun/kkamankun.github.io/assets/46318721/50380c2a-d648-4d3c-b5fc-96166d6af69b)

아래 그림은 RGB-NIR scene 데이터셋이며, 80 epoch까지 학습한 모델로 예측한 영상입니다.

![RGB-NIR scene test](https://github.com/kkamankun/kkamankun.github.io/assets/46318721/8f50868f-dc0a-4d9c-a39a-5b813de1b0ae){: width="50%"",height="50%""}

연구실에서 취득한 데이터셋으로 학습한 모델은 색 복원을 하지 못합니다. 반면, public 데이터셋으로 학습한 모델은 정답 RGB 영상과는 다르지만 그럴듯한 색을 복원하고 있습니다. 연구실에서 취득한 데이터셋이 public 데이터셋에 비해 overfitting이 빠르게 되고 색 복원을 하지 못하는 이유는 데이터셋의 양은 적은데 비해 다양성은 너무 높은 것으로 생각됩니다. 데이터셋의 다양성을 높이기 위해 scene을 구성하는 objects를 다양하게 구성했지만, 이에 따른 개수는 부족합니다. 그래서 모델이 일반화되지 못한 것으로 생각됩니다.

그동안은 모델이 데이터셋을 전부 외워버려 정답과 같은 예측 영상을 출력했던 것으로 생각합니다. 데이터셋을 외우는 성능을 비교하면 되기 때문에 알고리즘 연구는 가능할 수 있습니다. 그러나, 이 모델은 일반화 능력을 갖추지 못합니다. 이러한 이유로 연구실에서 취득한 데이터셋으로 RGB-to-NIR conversion을 수행하는 모델을 학습하여 public 데이터셋으로 NIR을 추정하고자 하면, NIR의 품질이 상당히 나쁩니다. 왜냐하면 모델은 일반화 능력을 갖추지 못했기 때문입니다.

이를 개선하기 위한 세 가지 방법을 생각했습니다. 첫째, 취득한 데이터셋의 다양성은 더 이상 높이지 않고 양을 늘려야 합니다. 데이터 증강 기법을 적용하거나 동일한 object에 대해 다양한 상황에서 추가적으로 취득해야 할 것입니다. 둘째, public 데이터셋에 없는 NIR 영상의 추정이 필요한 경우, public 데이터셋과 유사한 데이터셋을 취득하는 것입니다. 어떤 상황에서도 적용할 수 있는 데이터셋 취득에 시간과 비용이 많이 소요됨으로 target public 데이터셋과 최대한 유사하게 취득해야 할 것 같습니다. 그러나, public 데이터셋에는 ground truth가 없기 때문에 변환 성능을 평가할 방법이 없다는 문제가 있습니다. 마지막은 거대한 데이터셋을 구축하는 것입니다. 이때, 모델은 생성 모델로 구현합니다. 이를 통해, 정답 영상과 동일하게 만드는 모델이 아닌 정답스러운 영상을 생성하는 모델을 구현합니다. 그러나, 얼마나 많은 데이터셋을 취득해야 할지 모르고 생성 모델의 성능을 평가하는 지표가 없다는 문제가 있습니다.

따라서, 알고리즘 개발을 위해서는 데이터셋의 다양성을 제한하는 것이 필요합니다. 그러나 overfitting이 발생할 가능성이 있으므로 모델의 성능 변화를 관찰하기 어려울 수 있습니다. 또한 일반화 성능을 갖추지 못할 것입니다.

참고문헌
---
[1] [RGB-NIR scene dataset](https://www.epfl.ch/labs/ivrl/research/downloads/rgb-nir-scene-dataset/){: target="_blank"}

[2] [VCIP dataset](http://www.vcip2020.org/grand_challenge.htm){: target="_blank"}