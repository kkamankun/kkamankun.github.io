---
title: "셀프 어텐션(Self Attention)"
categories:
    - Deep Learning
# layout: default
use_math: true
---
트랜스포머(transformer)의 핵심 구성요소는 셀프 어텐션입니다. 셀프 어텐션은 트랜스포머의 인코더와 디코더 블록 모두에서 수행됩니다. 이 글에서는 인코더의 셀프 어텐션에 대해 살펴보겠습니다.

셀프 어텐션은 쿼리(query), 키(key), 밸류(value) 3개 요소 사이의 문맥적 관계성을 추출하는 과정입니다. 다음 수식처럼 입력 벡터 시퀀스(X)에 쿼리, 키, 밸류를 만들어주는 행렬(W)을 각각 곱합니다.

$Q=X*W_Q$

$K=X*W_K$

$V=X*W_V$

세 가지 행렬은 태스크를 가장 잘 수행하는 방향으로 학습 과정에서 업데이트됩니다.

셀프 어텐션의 정의입니다.

$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_K}})V$

위 수식을 말로 풀면 이렇습니다. 쿼리와 키를 행렬곱한 뒤 해당 행렬의 모든 요소값을 키 차원수의 제곱근 값으로 나눠주고, 이 행렬을 행(row) 단위로 소프트맥스(softmax)를 취해 스코어 행렬을 만들어줍니다. 이 스코어 행렬에 밸류를 행렬곱해 줘서 셀프 어텐션 계산을 마칩니다.

멀티-헤드 어텐션(Multi-Head Attention)은 셀프 어텐션(self attention)을 여러 번 수행한 걸 가리킵니다. 여러 헤드가 독자적으로 셀프 어텐션을 계산한다는 이야기입니다. 비유하자면 같은 문서(입력)을 두고 독자(헤드) 여러 명이 함께 읽는 구조라 할 수 있겠습니다. 멀티-헤드 어텐션은 개별 헤드의 셀프 어텐션 수행 결과를 이어붙인 행렬에  $W^O$를 행렬곱해서 마무리됩니다.  $W^O$의 크기는 ‘셀프 어텐션 수행 결과 행렬의 열(column)의 수 X 목표 차원수’가 됩니다.

이번에는 트랜스포머 인코더에서 수행하는 계산 과정을 셀프 어텐션을 중심으로 살펴보겠습니다. 인코더 블록의 입력은 이전 블록의 단어 벡터 시퀀스, 출력은 이번 블록 수행 결과로 도출된 단어 벡터 시퀀스입니다.

인코더에서 수행되는 셀프 어텐션은 쿼리, 키, 밸류가 모두 소스 시퀀스와 관련된 정보입니다. 결국 인코더에서 수행하는 셀프 어텐션은 소스 시퀀스 내의 모든 단어 쌍(pair) 사이의 관계를 고려하게 됩니다.

디코더에서 수행되는 셀프 어텐션을 순서대로 살펴보겠습니다. 우선 마스크 멀티 헤드 어텐션(Masked Multi-Head Attention)입니다. 이 모듈에서는 타깃 언어의 단어 벡터 시퀀스를 계산 대상으로 합니다. 이 파트에서는 입력 시퀀스가 타깃 언어로 바뀌었을 뿐 인코더 쪽 셀프 어텐션과 크게 다를 바가 없습니다.

그 다음은 멀티 헤드 어텐션입니다. 인코더와 디코더 쪽 정보를 모두 활용합니다. 인코더에서 넘어온 정보는 소스 언어의 문장의 단어 벡터 시퀀스입니다. 디코더 정보는 타깃 언어의 문장의 단어 벡터 시퀀스입니다. 전자를 키, 후자를 쿼리로 삼아 셀프 어텐션 계산을 수행합니다.

그런데 학습 과정에서는 약간의 트릭을 씁니다. 학습 과정에서 모델에 이번에 맞춰야 할 정답을 알려주게 되면 학습하는 의미가 사라집니다. 따라서 정답을 포함한 미래 정보를 셀프 어텐션 계산에서 제외하게 됩니다. 이 때문에 디코더 블록의 첫 번째 어텐션을 마스크 멀티-헤드 어텐션이라고 부릅니다. 마스킹은 확률이 0이 되도록 하여, 밸류와의 가중합에서 해당 단어 정보들이 무시되게끔 하는 방식으로 수행됩니다.

트랜스포머 모델은 이런 방식으로 말뭉치 전체를 훑어가면서 반복 학습합니다. 학습을 마친 모델은 다음처럼 기계 번역을 수행(인퍼런스)합니다.

1. 소스 언어(한국어) 문장을 인코더에 입력해 인코더 마지막 블록의 단어 벡터 시퀀스를 추출합니다.
2. 인코더에서 넘어온 소스 언어 문장 정보와 디코더에 타깃 문장 시작을 알리는 스페셜 토큰을 넣어서, 타깃 언어(영어)의 첫 번째 토큰을 생성합니다.
3. 인코더 쪽에서 넘어온 소스 언어 문장 정보와 이전에 생성된 타깃 언어 토큰 시퀀스를 디코더에 넣어서 만든 정보로 타깃 언어의 다음 토큰을 생성합니다.
4. 생성된 문장 길이가 충분하거나 문장 끝을 알리는 스페셜 토큰 </s>가 나올 때까지 3을 반복합니다.

한편 </s>는 보통 타깃 언어 문장 맨 마지막에 붙여서 학습합니다. 이 토큰이 나타났다는 것은 모델이 타깃 문장 생성을 마쳤다는 의미입니다.

---

참고문헌

[1] https://ratsgo.github.io/nlpbook/docs/language_model/transformers/